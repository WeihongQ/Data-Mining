{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC440 Programming Project\n",
    "## Weihong Qi\n",
    "### Oct.6 2022\n",
    "\n",
    "## 1 Introduction\n",
    "\n",
    "In this project, I develop and apply the Apriori and FP-Growth algorithms to the test data and evaluate the performance of both algorithms. For each algorithm, I firstly describe the algorithm in a natural language framework to show each step. Then I present the Python code to build and execute the main method of the algorithm. To test the performance of algorithms, I use the Use the UCI Adult Census Dataset (UCI Machine Learning Repository: Adult Data Set, n.d.) to test the methods and their running time. In the end, I compare the performance of the two algorithms.\n",
    "\n",
    "Section 2.1 and 2.2 of this report presents the algorithm, code and performance evaluation of Apriori. In Section 2.3, I develop the Hash-based Apriori and compare the efficiency between the two Apriori algorithms. Section 3 presents the algorithm, code and performance evaluation of the FP-Growth algorithm. I comprare the efficiency between Apriori and FP-Growth in Section 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from itertools import combinations\n",
    "from numpy import int32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Apriori\n",
    "The Apriori algorithm utilizes the Apriori property, which states as an itemset to be frequent, any of its non-empty subset must also be frquent. The algorithm involves a joint step and a prune step. Slightly differnt with Han, Kamber, and Pei (2012), in my algorithm, for each itemset, I prune the every subset first, and then check whetehr the itemset is frequent. Thus the algorithm to find a frequent  k-itemset is stated as follows:\n",
    "\n",
    "- Prune step: For each k-itemset in the data, for each non-empty seubset, check if the subset is in $L_1$, $L_2$, ..., $L_{k-1}$. If the subset is frequent, check the next subset; if the subset is not frequent, the k-item set is not frequent, and move to check the next k-itemset.\n",
    "- Scan all the tuples in the database and record the support counts of the k-item set. Check the whether the support count is greater than minimum support to determine whether this this itemset is frequent.\n",
    "\n",
    "In the next subsection, I present the code to execute the above algorithm. I test the algorithm with the UCI Adult Census Dataset.\n",
    "\n",
    "### 2.1 Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent 1-item set: \n",
      " {'United-States', 'Private', 'Male', 'White', '<=50K\\n', '0'}\n",
      "Frequent 2-item set: \n",
      " {\"['<=50K\\\\n', 'Private']\", \"['<=50K\\\\n', 'White']\", \"['United-States', 'White']\", \"['0', 'Male']\", \"['0', 'United-States']\", \"['<=50K\\\\n', 'United-States']\", \"['0', 'Private']\", \"['0', 'White']\", \"['Male', 'United-States']\", \"['Private', 'White']\", \"['0', '0']\", \"['Male', 'White']\", \"['0', '<=50K\\\\n']\", \"['Private', 'United-States']\"}\n",
      "Frequent 3-item set: \n",
      " {\"['0', '<=50K\\\\n', 'United-States']\", \"['0', '0', 'United-States']\", \"['<=50K\\\\n', 'United-States', 'White']\", \"['0', 'Male', 'White']\", \"['0', '0', '<=50K\\\\n']\", \"['0', '<=50K\\\\n', 'White']\", \"['Male', 'United-States', 'White']\", \"['0', '0', 'Male']\", \"['0', 'Private', 'White']\", \"['0', 'Private', 'United-States']\", \"['Private', 'United-States', 'White']\", \"['0', 'Male', 'United-States']\", \"['0', '0', 'White']\", \"['0', '<=50K\\\\n', 'Private']\", \"['0', '0', 'Private']\", \"['0', 'United-States', 'White']\"}\n"
     ]
    }
   ],
   "source": [
    "### Main method of Apriori ###\n",
    "\n",
    "def apriori(data, min_sup):\n",
    "    start_time = time.time()\n",
    "    # read data\n",
    "    with open(data) as df:\n",
    "        lines = df.readlines()\n",
    "    df.close()\n",
    "\n",
    "    n_lines = sum(1 for line in open(data))  # generate the number of lines\n",
    "    # print (\"There are \", n_lines, \"tuples in this data\")\n",
    "    # print (min_sup)\n",
    "\n",
    "    # frequent 1-itemset L1\n",
    "    # generate C1\n",
    "    supp_counts1 = defaultdict(int)\n",
    "\n",
    "    for line in lines:\n",
    "        for item in line.split(', '):\n",
    "            supp_counts1[item] += 1\n",
    "\n",
    "    # generate L1\n",
    "    frequent_item1 = set()\n",
    "    for index in supp_counts1:\n",
    "        if supp_counts1[index]/n_lines > min_sup:\n",
    "            frequent_item1.add(index)\n",
    "    print(\"Frequent 1-item set: \\n\", frequent_item1)\n",
    "\n",
    "    # frequent 2-itemset L2\n",
    "    supp_counts2 = defaultdict(int)\n",
    "\n",
    "    # generate C2 based on L1\n",
    "    for line in lines:\n",
    "        items = line.split(', ')\n",
    "        # find the frist possible item for the 2-item set\n",
    "        for index1 in range(len(items) - 1):\n",
    "            if items[index1] not in frequent_item1:\n",
    "                continue\n",
    "            # find the second possible item for the 2-item set\n",
    "            for index2 in range(index1 + 1, len(items)):\n",
    "                if items[index2] not in frequent_item1:\n",
    "                    continue\n",
    "                # arrange the items in alphabetic order\n",
    "                itemset2 = (items[index1], items[index2])\n",
    "                itemset2 = str(sorted(itemset2))\n",
    "                supp_counts2[itemset2] += 1\n",
    "\n",
    "    # generate L2\n",
    "    frequent_item2 = set()\n",
    "    for index in supp_counts2:\n",
    "        if supp_counts2[index]/n_lines > min_sup:\n",
    "            frequent_item2.add(index)\n",
    "\n",
    "    print(\"Frequent 2-item set: \\n\", frequent_item2)\n",
    "\n",
    "    #frequent 3-itemset L3\n",
    "    supp_counts3 = defaultdict(int)\n",
    "\n",
    "    # gernate C3 based on L2\n",
    "    for line in lines:\n",
    "        items = line.split(', ')\n",
    "        for index1 in range(len(items)-2):\n",
    "            if items[index1] not in frequent_item1:\n",
    "                continue\n",
    "            for index2 in range(index1+1, len(items)-1):\n",
    "                if items[index2] not in frequent_item1:\n",
    "                    continue\n",
    "                # combine the first two items into a group for 2-itemset frequent pattern check\n",
    "                index_group2_12 = (items[index1], items[index2])\n",
    "                index_group2_12 = str(sorted(index_group2_12))\n",
    "                # check if the 2-item set is frequent\n",
    "                if index_group2_12 not in frequent_item2:\n",
    "                    continue\n",
    "                for index3 in range(index2+1, len(items)):\n",
    "                    index_group2_13 = (items[index1], items[index3])\n",
    "                    index_group2_13 = str(sorted(index_group2_13))\n",
    "                    # check if the 2-itemset is frequent\n",
    "                    if index_group2_13 not in frequent_item2:\n",
    "                        continue\n",
    "                    # combine the second and the third items into a group for 2-itemset frequent pattern check\n",
    "                    index_group2_23 = (items[index2], items[index3])\n",
    "                    index_group2_23 = str(sorted(index_group2_23))\n",
    "\n",
    "                    # check if the 2-itemset if frequent\n",
    "                    if index_group2_23 not in frequent_item2:\n",
    "                        continue\n",
    "                    itemset3 = (items[index1], items[index2], items[index3])\n",
    "                    itemset3 = str(sorted(itemset3))\n",
    "                    supp_counts3[itemset3] += 1\n",
    "\n",
    "    # generate L3\n",
    "    frequent_item3 = set()\n",
    "    for index in supp_counts3:\n",
    "        if supp_counts3[index]/n_lines > min_sup:\n",
    "            frequent_item3.add(index)\n",
    "\n",
    "    print(\"Frequent 3-item set: \\n\", frequent_item3)\n",
    "\n",
    "#test the code\n",
    "apriori(\"adult.data\", min_sup=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the test, I set the minimum support as 0.5 and report the frequent itemset with 1, 2, and 3 items. The output shows that \"United States\", \"Private\", \"Male\", \"White\", \"<=50K\" and \"0\" (capital gain) is the requent 1-item set. In the 2-item set, there are many pairs that are meaningless in the sense to understand the relations between attributes. For instance, (\"Male\", \"White\") does not mean males are more likely to be white man. But some are potentially interesting, such as (\"<=50K\", \"Private\"), which means many people working in private sectors have income less than 50K. The 3-item set gives some more interesting results. For example, the (\"Private\", \"United-States\", \"White\") tuple can be explained as white American people have a large number of members working in private sectors, which can reflect some dimension of American employees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Efficiency of Apriori with different parameters\n",
    "\n",
    "In this subsection, I test the performance of the Apriori algorithm with different parameters and differnt sizes of dataset. I propose the following code chunk to track the running time of the Apriori algorithm without reporting specific results. The code is the same with the one in Section 2.1 exceptfor removing the \"print\" lines and adding a time tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the timer to evaluate apriori's efficiency\n",
    "def ap_timer(data, min_sup):\n",
    "    start_time = time.time()\n",
    "    # read data\n",
    "    with open(data) as df:\n",
    "        lines = df.readlines()\n",
    "    df.close()\n",
    "\n",
    "    n_lines = sum(1 for line in open(data))  # generate the number of lines\n",
    "    # print (\"There are \", n_lines, \"tuples in this data\")\n",
    "    # print (min_sup)\n",
    "\n",
    "    # frequent 1-itemset L1\n",
    "    # generate C1\n",
    "    supp_counts1 = defaultdict(int)\n",
    "\n",
    "    for line in lines:\n",
    "        for item in line.split(', '):\n",
    "            supp_counts1[item] += 1\n",
    "\n",
    "    # generate L1\n",
    "    frequent_item1 = set()\n",
    "    for index in supp_counts1:\n",
    "        if supp_counts1[index]/n_lines > min_sup:\n",
    "            frequent_item1.add(index)\n",
    "\n",
    "    # frequent 2-itemset L2\n",
    "    supp_counts2 = defaultdict(int)\n",
    "\n",
    "    # generate C2 based on L1\n",
    "    for line in lines:\n",
    "        items = line.split(', ')\n",
    "        # find the frist possible item for the 2-itemset\n",
    "        for index1 in range(len(items) - 1):\n",
    "            if items[index1] not in frequent_item1:\n",
    "                continue\n",
    "            # find the second possible item for the 2-itemset\n",
    "            for index2 in range(index1 + 1, len(items)):\n",
    "                if items[index2] not in frequent_item1:\n",
    "                    continue\n",
    "                # arrange the items in alphabetic order\n",
    "                itemset2 = (items[index1], items[index2])\n",
    "                itemset2 = str(sorted(itemset2))\n",
    "                supp_counts2[itemset2] += 1\n",
    "\n",
    "    # generate L2\n",
    "    frequent_item2 = set()\n",
    "    for index in supp_counts2:\n",
    "        if supp_counts2[index]/n_lines > min_sup:\n",
    "            frequent_item2.add(index)\n",
    "\n",
    "\n",
    "    #frequent 3-itemset L3\n",
    "    supp_counts3 = defaultdict(int)\n",
    "\n",
    "    # gernate C3 based on L2\n",
    "    for line in lines:\n",
    "        items = line.split(', ')\n",
    "        for index1 in range(len(items)-2):\n",
    "            if items[index1] not in frequent_item1:\n",
    "                continue\n",
    "            for index2 in range(index1+1, len(items)-1):\n",
    "                if items[index2] not in frequent_item1:\n",
    "                    continue\n",
    "                # combine the first two items into a group for 2-itemset frequent pattern check\n",
    "                index_group2_12 = (items[index1], items[index2])\n",
    "                index_group2_12 = str(sorted(index_group2_12))\n",
    "                # check if the 2-itemset is frequent\n",
    "                if index_group2_12 not in frequent_item2:\n",
    "                    continue\n",
    "                for index3 in range(index2+1, len(items)):\n",
    "                    index_group2_13 = (items[index1], items[index3])\n",
    "                    index_group2_13 = str(sorted(index_group2_13))\n",
    "                    # check if the 2-itemset is frequent\n",
    "                    if index_group2_13 not in frequent_item2:\n",
    "                        continue\n",
    "                    # combine the second and the third items into a group for 2-itemset frequent pattern check\n",
    "                    index_group2_23 = (items[index2], items[index3])\n",
    "                    index_group2_23 = str(sorted(index_group2_23))\n",
    "\n",
    "                    # check if the 2-itemset if frequent\n",
    "                    if index_group2_23 not in frequent_item2:\n",
    "                        continue\n",
    "                    itemset3 = (items[index1], items[index2], items[index3])\n",
    "                    itemset3 = str(sorted(itemset3))\n",
    "                    supp_counts3[itemset3] += 1\n",
    "\n",
    "    # generate L3\n",
    "    frequent_item3 = set()\n",
    "    for index in supp_counts3:\n",
    "        if supp_counts3[index]/n_lines > min_sup:\n",
    "            frequent_item3.add(index)\n",
    "\n",
    "    print(\"--- %.6f seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 7.808020 seconds ---\n",
      "--- 3.399386 seconds ---\n",
      "--- 1.416005 seconds ---\n",
      "--- 0.032830 seconds ---\n",
      "--- 0.356331 seconds ---\n",
      "--- 3.368015 seconds ---\n"
     ]
    }
   ],
   "source": [
    "ap_timer(\"adult.data\", min_sup=0.3)\n",
    "ap_timer(\"adult.data\", min_sup=0.5)\n",
    "ap_timer(\"adult.data\", min_sup=0.7)\n",
    "\n",
    "ap_timer(\"adult_1p.data\", min_sup=0.5)\n",
    "ap_timer(\"adult_10p.data\", min_sup=0.5)\n",
    "ap_timer(\"adult.data\", min_sup=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I test the algorithm with the same data but different minimum support. The result shows that the running time is 7.8 seconds, 3.4 seconds and 1.4 seconds for minimum support 0.3, 0.5 and 0.7 respectively. The running time decreases as minimum support increases, because greater minumum support makes it throw away mroe candidates before scannding the whole dataset for the k-item set.\n",
    "\n",
    "Alternatively, I test the performance of the algorithm with different data size. With 1%, 10% and the complete data, the running time is 0.03 seconds, 0.36 seconds and 3.37 seconds respectively. As the data size increases, the running time also increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Improved Apriori: using Hash-based technique\n",
    "\n",
    "In this subssection, I propose the improved Apriori using hash-based technique. The hash-based technique distribute all possible candidate sets for k-item set into different buckets using hash formula while generating $L_{k-1}$. It utilizes the possible duplication in hash table to reduce the number of candidate set in the next scan. Note that although Han, Kamber, and Pei (2012) uses 7 as the hash parameter, there is no definite rule to pick an optimal parameter. In general, a too small parameter makes the algorithm useless because it cannot eliminate any of the candidate sets. In my practice, I tested several parameters and selected 59 when the minimum support is 0.3 in this example. The hash parameter helps to cut the number of candidates from 78 to 59. \n",
    "\n",
    "The code and the results are reported below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of C2 is 78\n",
      "the length of H2 is 59\n",
      "{\"['0', 'Never-married']\", \"['<=50K\\\\n', 'White']\", \"['Male', 'Private']\", \"['Husband', 'United-States']\", \"['40', 'Male']\", \"['0', '<=50K\\\\n']\", \"['Married-civ-spouse', 'White']\", \"['0', 'Married-civ-spouse']\", \"['0', 'Male']\", \"['<=50K\\\\n', 'Never-married']\", \"['<=50K\\\\n', 'Male']\", \"['40', 'White']\", \"['Male', 'United-States']\", \"['Married-civ-spouse', 'United-States']\", \"['0', 'Female']\", \"['Husband', 'White']\", \"['0', '9']\", \"['9', 'HS-grad']\", \"['0', 'Private']\", \"['<=50K\\\\n', 'United-States']\", \"['40', '<=50K\\\\n']\", \"['0', '40']\", \"['0', '0']\", \"['Private', 'United-States']\", \"['40', 'Private']\", \"['<=50K\\\\n', 'Private']\", \"['Husband', 'Male']\", \"['0', 'United-States']\", \"['United-States', 'White']\", \"['40', 'United-States']\", \"['Private', 'White']\", \"['Husband', 'Married-civ-spouse']\", \"['0', 'Husband']\", \"['0', 'White']\", \"['0', 'HS-grad']\", \"['Male', 'Married-civ-spouse']\", \"['Male', 'White']\"}\n",
      "--- 3.438760 seconds ---\n"
     ]
    }
   ],
   "source": [
    "def hash_ap(data, min_sup):\n",
    "    start_time = time.time()\n",
    "\n",
    "    with open(data) as df:\n",
    "        lines = df.readlines()\n",
    "    df.close()\n",
    "\n",
    "    n_lines = sum(1 for line in open(data)) \n",
    "\n",
    "    supp_counts1 = defaultdict(int)\n",
    "    H_index = defaultdict(int)\n",
    "    hashTable = defaultdict(int32)\n",
    "\n",
    "    for line in lines:\n",
    "        for item in line.split(', '):\n",
    "            supp_counts1[item] += 1\n",
    "\n",
    "    # find frequent candidate for 1-itemset\n",
    "    frequent_item1 = set()\n",
    "    for index in supp_counts1:\n",
    "        if supp_counts1[index]/n_lines > min_sup:\n",
    "            frequent_item1.add(index)\n",
    "\n",
    "\n",
    "    #Generate all 2-itemset based on L1\n",
    "    L1 = list(frequent_item1)\n",
    "    C2 = combinations(L1, 2)\n",
    "    C2 = list(C2) #the length of C2 is 78\n",
    "\n",
    "    #generate hash table and increase corresponding bucket counts\n",
    "    for itemset in C2:\n",
    "        item1 = itemset[0]\n",
    "        index1 = L1.index(item1)\n",
    "        item2 = itemset[1]\n",
    "        index2 = L1.index(item2)\n",
    "        #calculate hash index/address with hash function\n",
    "        hindex = (index1*10 + index2) % 59\n",
    "        H_index[itemset] = hindex #the dictionary store the itemset and corresponding index\n",
    "\n",
    "\n",
    "        for line in lines:\n",
    "            if itemset[0] in line and itemset[1] in line: \n",
    "                hashTable[hindex] += 1\n",
    "\n",
    "    # generate a reduced 2-itemset for scan in the second pass\n",
    "    candidate_hash2 = set()\n",
    "    for key in hashTable:\n",
    "        if hashTable[key]/n_lines > min_sup:\n",
    "            candidate_hash2.add(key)\n",
    "    #print(hashTable)\n",
    "\n",
    "\n",
    "    H2 = set()\n",
    "    for key in H_index:\n",
    "        index = H_index.get(key)\n",
    "        if index in candidate_hash2:\n",
    "            H2.add(key)\n",
    "        \n",
    "    print(\"the length of C2 is\", len(C2))\n",
    "    print(\"the length of H2 is\", len(H2))\n",
    "\n",
    "    supp_counts2= defaultdict(int) #the dictionary to track the counts of 2-itemset\n",
    "\n",
    "    #find the frequent 2-itemset candidates\n",
    "    for line in lines:\n",
    "        items = line.split(', ')\n",
    "        #find the frist possible item for the 2-itemset\n",
    "        for index1 in range(len(items) -1): \n",
    "            if items[index1] not in frequent_item1:\n",
    "                continue\n",
    "            #find the second possible item for the 2-itemset\n",
    "            for index2 in range(index1+ 1, len(items)):\n",
    "                if items[index2] not in frequent_item1:\n",
    "                    continue\n",
    "                #arrange the items in alphabetic order\n",
    "                itemset2 = (items[index1], items[index2])\n",
    "                itemset2 = str(sorted(itemset2))\n",
    "                supp_counts2[itemset2] += 1\n",
    "\n",
    "    #find the frequent 2-itemset\n",
    "    frequent_item2 = set()\n",
    "    for index in supp_counts2:\n",
    "        if supp_counts2[index]/n_lines > min_sup:\n",
    "            frequent_item2.add(index)\n",
    "\n",
    "    print(frequent_item2)\n",
    "    print(\"--- %.6f seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "hash_ap(\"adult.data\", min_sup=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the efficiency of the two Apriori algorithms, similar to section 2.2, I propose the following timer for hash-based Apriori. Then I run several tests with differnt parameters to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_timer(data, min_sup):\n",
    "    start_time = time.time()\n",
    "\n",
    "    with open(data) as df:\n",
    "        lines = df.readlines()\n",
    "    df.close()\n",
    "\n",
    "    n_lines = sum(1 for line in open(data)) \n",
    "\n",
    "    supp_counts1 = defaultdict(int)\n",
    "    H_index = defaultdict(int)\n",
    "    hashTable = defaultdict(int32)\n",
    "\n",
    "    for line in lines:\n",
    "        for item in line.split(', '):\n",
    "            supp_counts1[item] += 1\n",
    "\n",
    "    # find frequent candidate for 1-itemset\n",
    "    frequent_item1 = set()\n",
    "    for index in supp_counts1:\n",
    "        if supp_counts1[index]/n_lines > min_sup:\n",
    "            frequent_item1.add(index)\n",
    "\n",
    "\n",
    "    #Generate all 2-itemset based on L1\n",
    "    L1 = list(frequent_item1)\n",
    "    C2 = combinations(L1, 2)\n",
    "    C2 = list(C2) #the length of C2 is 78\n",
    "\n",
    "    #generate hash table and increase corresponding bucket counts\n",
    "    for itemset in C2:\n",
    "        item1 = itemset[0]\n",
    "        index1 = L1.index(item1)\n",
    "        item2 = itemset[1]\n",
    "        index2 = L1.index(item2)\n",
    "        #calculate hash index/address with hash function\n",
    "        hindex = (index1*10 + index2) % 59\n",
    "        H_index[itemset] = hindex #the dictionary store the itemset and corresponding index\n",
    "\n",
    "\n",
    "        for line in lines:\n",
    "            if itemset[0] in line and itemset[1] in line: \n",
    "                hashTable[hindex] += 1\n",
    "\n",
    "    # generate a reduced 2-itemset for scan in the second pass\n",
    "    candidate_hash2 = set()\n",
    "    for key in hashTable:\n",
    "        if hashTable[key]/n_lines > min_sup:\n",
    "            candidate_hash2.add(key)\n",
    "    #print(hashTable)\n",
    "\n",
    "\n",
    "    H2 = set()\n",
    "    for key in H_index:\n",
    "        index = H_index.get(key)\n",
    "        if index in candidate_hash2:\n",
    "            H2.add(key)\n",
    "        \n",
    "    #print(\"the length of C2 is\", len(C2))\n",
    "    #print(\"the length of H2 is\", len(H2))\n",
    "\n",
    "    supp_counts2= defaultdict(int) #the dictionary to track the counts of 2-itemset\n",
    "\n",
    "    #find the frequent 2-itemset candidates\n",
    "    for line in lines:\n",
    "        items = line.split(', ')\n",
    "        #find the frist possible item for the 2-itemset\n",
    "        for index1 in range(len(items) -1): \n",
    "            if items[index1] not in frequent_item1:\n",
    "                continue\n",
    "            #find the second possible item for the 2-itemset\n",
    "            for index2 in range(index1+ 1, len(items)):\n",
    "                if items[index2] not in frequent_item1:\n",
    "                    continue\n",
    "                #arrange the items in alphabetic order\n",
    "                itemset2 = (items[index1], items[index2])\n",
    "                itemset2 = str(sorted(itemset2))\n",
    "                supp_counts2[itemset2] += 1\n",
    "\n",
    "    #find the frequent 2-itemset\n",
    "    frequent_item2 = set()\n",
    "    for index in supp_counts2:\n",
    "        if supp_counts2[index]/n_lines > min_sup:\n",
    "            frequent_item2.add(index)\n",
    "\n",
    "    #print(frequent_item2)\n",
    "    print(\"--- %.6f seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.402912 seconds ---\n",
      "--- 0.879161 seconds ---\n",
      "--- 0.615119 seconds ---\n"
     ]
    }
   ],
   "source": [
    "hash_timer(\"adult.data\", min_sup=0.3)\n",
    "hash_timer(\"adult.data\", min_sup=0.5)\n",
    "hash_timer(\"adult.data\", min_sup=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the running time of Apriori in section 2.2 and the running time of hash-based Apriori in the section, It is shown that the efficiency of hash-based Apriori is better than the original one.\n",
    "\n",
    "![image](Figure1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 FP-Growth\n",
    "\n",
    "In this section, I describe the FP-Growth algorithm, code and the performance evaluation. FP-Growth borrows the data structure of trees to avoid generateting a large pool of candidates. The algorithm of FP-Growth can be described as follows:\n",
    "\n",
    "1. Find frequent items\n",
    "   - Find all frequent 1-item set. \n",
    "   - Sort all the frequent 1-item set in frequency in descending order, at the same time, delete the items whose frequency is smaller than min_sup\n",
    "   - For each tuple, reorder the items by the order in the sorted frequent 1-item set\n",
    "\n",
    "2. Construct FP tree\n",
    "   - Create a node object to track the position of items\n",
    "   - Create a tree object with root = “null”. \n",
    "   Fill the tree with frequent items and their support counts, and assign each node a key\n",
    "   For each line in the data, sort the items in the order of sorted frequent items\n",
    "   Create a node table to track the order of node\n",
    "\n",
    "3. Generate conditional FP tree\n",
    "   - Given the FP tree and the stored nodes, construct a conditional FP tree\n",
    "   - Construct a set containing all the suffix by the order of nodes\n",
    "\n",
    "4. Generate all frequent patterns based on conditional FP tree\n",
    "\n",
    "The code to apply the FP-Growth algorithm is presented in 3.1. To test the algorithm, I use the 10% subset of UCI Adult Census Dataset. I did not use the complete dataset becasue FP-Growth finds all frequent itemsets until hit the maximum length of all attributes. Thus it can take a long time to test the function. Of the purpose to test the function and performance efficiently, I use the 10% subset of the data.\n",
    "\n",
    "\n",
    "### 3.1 Main method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[('<=50K\\n', 'White', 'United-States', '0', 'Private')]\n",
      "[('<=50K\\n', 'White', 'United-States', 'Private'), ('<=50K\\n', 'White', '0', 'Private'), ('<=50K\\n', 'United-States', '0', 'Private'), ('White', 'United-States', '0', 'Private')]\n",
      "[('White', 'United-States', '0', 'Male')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.FPtree at 0x7fcc5a9fcdc0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "def readdata(data):\n",
    "    with open(data) as df:\n",
    "        newdata = []\n",
    "        lines = df.readlines()\n",
    "        for line in lines:\n",
    "            newdata.append(line.split(\", \"))\n",
    "    df.close()\n",
    "    return newdata\n",
    "\n",
    "# test data\n",
    "min_sup = 0.5\n",
    "\n",
    "test_data2 = readdata(\"adult_10p.data\")\n",
    "obs = sum(1 for line in open(\"adult_10p.data\"))\n",
    "\n",
    "\n",
    "# Begin FP-Groth Algorithm\n",
    "# construct the node class, which stores the item, support counts, children node and parent node\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, item, item_count=0, parent=None):\n",
    "        self.item = item\n",
    "        self.item_count = item_count\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "\n",
    "# construct the FP tree\n",
    "\n",
    "\n",
    "class FPtree:\n",
    "    def __init__(self, data, min_sup, obs):\n",
    "        self.data = data\n",
    "        self.min_sup = min_sup\n",
    "        self.root = Node(item='null')\n",
    "        self.obs = obs\n",
    "\n",
    "\n",
    "        nodeTable = []\n",
    "\n",
    "        # find all the frequent 1-itemset in the data\n",
    "        sup_counts = defaultdict(int)\n",
    "        freqitem = set()\n",
    "        for line in data:\n",
    "            for items in line:\n",
    "                sup_counts[items] += 1\n",
    "\n",
    "        for item in sup_counts:\n",
    "            if sup_counts[item]/obs >= self.min_sup:\n",
    "                freqitem.add(item)\n",
    "        # sort the frequent items\n",
    "        freqSort = sorted(sup_counts.items(), key=lambda x: (-x[1], x[0]))\n",
    "        for i in freqSort:\n",
    "            if i[0] not in freqitem:\n",
    "                freqSort.remove(i)\n",
    "\n",
    "        # construct the FP tree\n",
    "        # sort the items in each line\n",
    "        for line in data:\n",
    "            lineSort = []\n",
    "            for i in freqSort:\n",
    "                if i[0] in line:\n",
    "                    lineSort.append(i[0])\n",
    "            line = lineSort\n",
    "            # print(lineSort)\n",
    "            # insert the items into the FP tree\n",
    "            R = self.root\n",
    "            for item in line:\n",
    "                if item not in R.children.keys():\n",
    "                    R.children[item] = Node(\n",
    "                        item=item, item_count=1, parent=R)\n",
    "                else:\n",
    "                    R.children[item].item_count += 1\n",
    "                    R.children[item].parent = R\n",
    "\n",
    "                R = R.children[item]\n",
    "                nodeTable.append(R)\n",
    "\n",
    "        # construct conditional tree\n",
    "        # find the largest key and starting from the node with this key\n",
    "\n",
    "        def condline(N):\n",
    "            condfpline = []\n",
    "            if N.parent == None:\n",
    "                return None\n",
    "            while N.parent.item != 'null':\n",
    "                P = N.parent\n",
    "                condfpline.append(P.item)\n",
    "                N = N.parent\n",
    "            return condfpline\n",
    "\n",
    "        condset = []\n",
    "        for node in nodeTable:\n",
    "            n = condline(node)\n",
    "            condset.append(n)\n",
    "\n",
    "        # find all frequent pattern\n",
    "        freq_patt = []\n",
    "        for item in freqSort:\n",
    "            condtreeset = []\n",
    "            i = 0\n",
    "            for node in nodeTable:\n",
    "                if node.item == item[0]:\n",
    "                    condtreeset.append(condset[i])\n",
    "                i += 1\n",
    "            # print(condtreeset)\n",
    "\n",
    "            cdcount = defaultdict(int)\n",
    "            condFP = []\n",
    "            for line in condtreeset:\n",
    "                for cand in line:\n",
    "                    cdcount[cand] += 1\n",
    "            for cand in cdcount:\n",
    "                if cdcount[cand]/obs >= min_sup:\n",
    "                    condFP.append(cand)\n",
    "            if len(condFP):\n",
    "                condFP.append(item[0])\n",
    "               # print(condFP)\n",
    "                c = len(condFP)\n",
    "                while c > 3:\n",
    "                    pattern = list(itertools.combinations(condFP, c))\n",
    "                    for t in pattern:\n",
    "                        if item[0] not in str(t):\n",
    "                            pattern.remove(t)\n",
    "                    print(pattern)\n",
    "                    # freq_patt.append(pattern)\n",
    "                    c = c - 1\n",
    "\n",
    "\n",
    "FPtree(test_data2, min_sup, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set the minimum support to be 0.5 to test the FP-algorithm. It returns all the frequent itemset in the output. Some of the results reveal possible correlations among attributes. For example, ('<=50K\\n', 'United-States', '0', 'Private') suggests there are many Americans working in private sectors having income less than 50K and 0 capital gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Efficiency of FP-Growth with different parameters\n",
    "To evaluate the efficiency of FP-Growth algorithm, similar to section 2.2 and 2.3, I construct a timer to track the running time of FP-growth with different parameters. To save the space, I put the code and the output in the Appendix. The results show that the running time is 13.44 seconds, 11.45 seconds and 10.14 seconds with the minimum support equals to 0.1, 0.3, and 0.6 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Compare Apriori and FP-Growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I compare the performance of the Apriori, Hash-based Apriori and the FP-Growth. I did not use the UCI Adult Census Database, however, because while Apriori finds frequent itemset until 3-item set, the FP-Growth finds all frequent itemset. Because there are many attributes and tuples in the UCI Adult Census Database, the output are compeltely different for the two algorithms. To avoid comparing apples with oranges, I use the data used in Han, Kamber, and Pei (2012). The output of this data is the same for all the algorothms. Although the dataset is quite small, it is still enough to reveal the efficiency of different algotithms. The specific dataset is shown in Appendix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.000267 seconds ---\n",
      "--- 0.000219 seconds ---\n",
      "--- 0.000179 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "FPtree(\"test.data\", min_sup=2/9, obs=9)\n",
    "print(\"--- %.6f seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "FPtree(\"test.data\", min_sup=5/9, obs=9)\n",
    "print(\"--- %.6f seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "FPtree(\"test.data\", min_sup=7/9, obs=9)\n",
    "print(\"--- %.6f seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.002349 seconds ---\n",
      "--- 0.001937 seconds ---\n",
      "--- 0.000260 seconds ---\n"
     ]
    }
   ],
   "source": [
    "ap_timer(\"test.data\", min_sup=2/9)\n",
    "ap_timer(\"test.data\", min_sup=5/9)\n",
    "ap_timer(\"test.data\", min_sup=7/9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.001444 seconds ---\n",
      "--- 0.000842 seconds ---\n",
      "--- 0.001042 seconds ---\n"
     ]
    }
   ],
   "source": [
    "hash_timer(\"test.data\", min_sup=2/9)\n",
    "hash_timer(\"test.data\", min_sup=5/9)\n",
    "hash_timer(\"test.data\", min_sup=7/9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I combine the results of the three timers to show the efficiency of the three algorithms using the timers. Figure 2 shows the effciency comparison of the algorithms.\n",
    "\n",
    "![image](Figure2.png)\n",
    "\n",
    "According to the figure, FP-Growth is much faster than Apriori and Hash-based Apriori. This is due to the lower computational complexity of FP-Growth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this programming project, I practice the application of Apriori, Hash-based Apriori and the FP-Growth algorithm using the UCI Adult Census Database. The output reports some results that may reflect correlations among American people's demographic features. The report also presents the performance of the three algorithms. In general, Hash-based Apriori reduces the running time significantly and FP-Growth is much more efficient than Apriori. This implies FP-Growth should be optimal in general cases rather than Apriori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "Han, Kamber, M., & Pei, J. (2012). Data mining concepts and techniques / Jiawei Han, Micheline Kamber, Jian Pei. (3rd ed.). Elsevier.\n",
    "\n",
    "UCI Machine Learning Repository: Adult Data Set. (n.d.). Retrieved October 6, 2022, from http://archive.ics.uci.edu/ml/datasets/Adult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 13.438151 seconds ---\n",
      "--- 11.453575 seconds ---\n",
      "--- 10.137549 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "def readdata(data):\n",
    "    with open(data) as df:\n",
    "        newdata = []\n",
    "        lines = df.readlines()\n",
    "        for line in lines:\n",
    "            newdata.append(line.split(\", \"))\n",
    "    df.close()\n",
    "    return newdata\n",
    "\n",
    "# test data\n",
    "\n",
    "\n",
    "test_data = [['I1', 'I2', 'I5'],\n",
    "             ['I2', 'I4'],\n",
    "             ['I2', 'I3'],\n",
    "             ['I1', 'I2', 'I4'],\n",
    "             ['I1', 'I3'],\n",
    "             ['I2', 'I3'],\n",
    "             ['I1', 'I3'],\n",
    "             ['I1', 'I2', 'I3', 'I5'],\n",
    "             ['I1', 'I2', 'I3', 'I6']]\n",
    "\n",
    "test_data2 = readdata(\"adult_10p.data\")\n",
    "obs = sum(1 for line in open(\"adult_10p.data\"))\n",
    "\n",
    "\n",
    "# Begin FP-Groth Algorithm\n",
    "# construct the node class, which stores the item, support counts, children node and parent node\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, item, item_count=0, parent=None):\n",
    "        self.item = item\n",
    "        self.item_count = item_count\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "\n",
    "# construct the FP tree\n",
    "\n",
    "\n",
    "class FPtimer:\n",
    "    def __init__(self, data, min_sup, obs):\n",
    "        self.data = data\n",
    "        self.min_sup = min_sup\n",
    "        self.root = Node(item='null')\n",
    "        self.obs = obs\n",
    "\n",
    "\n",
    "        nodeTable = []\n",
    "\n",
    "        # find all the frequent 1-itemset in the data\n",
    "        sup_counts = defaultdict(int)\n",
    "        freqitem = set()\n",
    "        for line in data:\n",
    "            for items in line:\n",
    "                sup_counts[items] += 1\n",
    "\n",
    "        for item in sup_counts:\n",
    "            if sup_counts[item]/obs >= self.min_sup:\n",
    "                freqitem.add(item)\n",
    "        # sort the frequent items\n",
    "        freqSort = sorted(sup_counts.items(), key=lambda x: (-x[1], x[0]))\n",
    "        for i in freqSort:\n",
    "            if i[0] not in freqitem:\n",
    "                freqSort.remove(i)\n",
    "\n",
    "        # construct the FP tree\n",
    "        # sort the items in each line\n",
    "        for line in data:\n",
    "            lineSort = []\n",
    "            for i in freqSort:\n",
    "                if i[0] in line:\n",
    "                    lineSort.append(i[0])\n",
    "            line = lineSort\n",
    "            # print(lineSort)\n",
    "            # insert the items into the FP tree\n",
    "            R = self.root\n",
    "            for item in line:\n",
    "                if item not in R.children.keys():\n",
    "                    R.children[item] = Node(\n",
    "                        item=item, item_count=1, parent=R)\n",
    "                else:\n",
    "                    R.children[item].item_count += 1\n",
    "                    R.children[item].parent = R\n",
    "\n",
    "                R = R.children[item]\n",
    "                nodeTable.append(R)\n",
    "\n",
    "        # construct conditional tree\n",
    "        # find the largest key and starting from the node with this key\n",
    "\n",
    "        def condline(N):\n",
    "            condfpline = []\n",
    "            if N.parent == None:\n",
    "                return None\n",
    "            while N.parent.item != 'null':\n",
    "                P = N.parent\n",
    "                condfpline.append(P.item)\n",
    "                N = N.parent\n",
    "            return condfpline\n",
    "\n",
    "        condset = []\n",
    "        for node in nodeTable:\n",
    "            n = condline(node)\n",
    "            condset.append(n)\n",
    "\n",
    "        # find all frequent pattern\n",
    "        freq_patt = []\n",
    "        for item in freqSort:\n",
    "            condtreeset = []\n",
    "            i = 0\n",
    "            for node in nodeTable:\n",
    "                if node.item == item[0]:\n",
    "                    condtreeset.append(condset[i])\n",
    "                i += 1\n",
    "            # print(condtreeset)\n",
    "\n",
    "            cdcount = defaultdict(int)\n",
    "            condFP = []\n",
    "            for line in condtreeset:\n",
    "                for cand in line:\n",
    "                    cdcount[cand] += 1\n",
    "            for cand in cdcount:\n",
    "                if cdcount[cand]/obs >= min_sup:\n",
    "                    condFP.append(cand)\n",
    "            if len(condFP):\n",
    "                condFP.append(item[0])\n",
    "               # print(condFP)\n",
    "                c = len(condFP)\n",
    "                while c > 3:\n",
    "                    pattern = list(itertools.combinations(condFP, c))\n",
    "                    for t in pattern:\n",
    "                        if item[0] not in str(t):\n",
    "                            pattern.remove(t)\n",
    "                    #print(pattern)\n",
    "                    # freq_patt.append(pattern)\n",
    "                    c = c - 1\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "min_sup = 0.1\n",
    "FPtimer(test_data2, min_sup, obs=obs)\n",
    "print(\"--- %.6f seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "min_sup = 0.3\n",
    "FPtimer(test_data2, min_sup, obs=obs)\n",
    "print(\"--- %.6f seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "min_sup = 0.6\n",
    "FPtimer(test_data2, min_sup, obs=obs)\n",
    "print(\"--- %.6f seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sup = 2/9\n",
    "\n",
    "test_data = [['I1', 'I2', 'I5'],\n",
    "             ['I2', 'I4'],\n",
    "             ['I2', 'I3'],\n",
    "             ['I1', 'I2', 'I4'],\n",
    "             ['I1', 'I3'],\n",
    "             ['I2', 'I3'],\n",
    "             ['I1', 'I3'],\n",
    "             ['I1', 'I2', 'I3', 'I5'],\n",
    "             ['I1', 'I2', 'I3', 'I6']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48ea76dd6cf8630a08eb7fdfa24a3121260d9c6db7a5ca0ae18ed406dd246da3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
